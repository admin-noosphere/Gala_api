#!/usr/bin/env python3
"""
API avec GPU 1 libre
"""

import os
import sys
import socket
import logging
import warnings
import numpy as np
import wave
import io
warnings.filterwarnings("ignore")

from flask import Flask, request, jsonify
from datetime import datetime

# Configuration GPU 1 (libre)
os.environ["CUDA_VISIBLE_DEVICES"] = "1"

import torch

# Paths
neurosync_path = "/home/gieidi-prime/Agents/NeuroSync_Local_API/neurosync_v3_all copy/NeuroSync_Real-Time_API"
sys.path.insert(0, neurosync_path)

# Imports NeuroSync
from models.neurosync.config import config
from models.neurosync.generate_face_shapes import generate_facial_data_from_bytes
from models.neurosync.model.model import load_model

# Module LiveLink
sys.path.append('/home/gieidi-prime/Agents/Claude/Gala_v1')
from modules.pylivelinkface import PyLiveLinkFace, FaceBlendShape

# Configuration
LIVELINK_IP = "192.168.1.14"
LIVELINK_PORT = 11111
API_PORT = 6969

# Logging minimal
logging.basicConfig(level=logging.INFO, format='%(levelname)s | %(message)s')
logger = logging.getLogger(__name__)

# Flask app
app = Flask(__name__)

# Variables globales
blendshape_model = None
py_face = None
socket_connection = None

def create_wav_from_pcm(pcm_data, sample_rate=16000):
    """Cr√©e un fichier WAV √† partir de donn√©es PCM brutes"""
    wav_buffer = io.BytesIO()
    
    with wave.open(wav_buffer, 'wb') as wav_file:
        wav_file.setnchannels(1)  # Mono
        wav_file.setsampwidth(2)  # 16-bit
        wav_file.setframerate(sample_rate)
        wav_file.writeframes(pcm_data)
    
    wav_buffer.seek(0)
    return wav_buffer.read()

def process_audio_input(audio_bytes):
    """Traite l'audio d'entr√©e et s'assure qu'il est au format WAV"""
    # V√©rifier si c'est d√©j√† du WAV
    if audio_bytes.startswith(b'RIFF'):
        return audio_bytes
    
    # Sinon, c'est probablement du PCM brut
    logger.info("Audio PCM brut d√©tect√©, conversion en WAV")
    return create_wav_from_pcm(audio_bytes, 16000)

def load_neurosync_model():
    """Charge le mod√®le NeuroSync"""
    global blendshape_model
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"Chargement du mod√®le NeuroSync sur {device} (GPU {os.environ.get('CUDA_VISIBLE_DEVICES', 'default')})")
    
    model_path = os.path.join(neurosync_path, 'models/neurosync/model/model.pth')
    blendshape_model = load_model(model_path, config, device)
    
    logger.info("‚úÖ Mod√®le charg√©")
    return blendshape_model

def init_livelink():
    """Initialise la connexion LiveLink"""
    global py_face, socket_connection
    
    logger.info(f"Connexion LiveLink vers {LIVELINK_IP}:{LIVELINK_PORT}")
    
    py_face = PyLiveLinkFace(name="GalaFace", fps=60)
    socket_connection = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    socket_connection.connect((LIVELINK_IP, LIVELINK_PORT))
    
    logger.info("‚úÖ LiveLink connect√©")

def send_to_livelink_fast(blendshapes):
    """Envoi rapide √† LiveLink"""
    global py_face, socket_connection
    
    if not py_face or not socket_connection:
        return
    
    try:
        # Reset
        for i in range(52):
            py_face._blend_shapes[i] = 0.0
        
        # Appliquer les valeurs
        for i in range(min(52, len(blendshapes))):
            value = blendshapes[i]
            if value > 0.001:
                py_face._blend_shapes[i] = max(0.0, min(1.0, value))
        
        # Envoyer
        socket_connection.sendall(py_face.encode())
        
    except Exception as e:
        logger.error(f"Erreur LiveLink: {e}")

@app.route('/health', methods=['GET'])
def health():
    """Endpoint de sant√©"""
    return jsonify({
        "status": "healthy",
        "model_loaded": blendshape_model is not None,
        "livelink_connected": socket_connection is not None,
        "gpu": os.environ.get('CUDA_VISIBLE_DEVICES', 'default')
    })

@app.route('/audio_to_blendshapes', methods=['POST'])
def audio_to_blendshapes_route():
    """Endpoint principal avec gestion audio fix√©e"""
    try:
        # R√©cup√©rer les donn√©es audio
        audio_bytes = request.data
        
        if not audio_bytes:
            return jsonify({"status": "error", "message": "No audio data"}), 400
        
        # Convertir l'audio
        wav_data = process_audio_input(audio_bytes)
        
        # Traitement des blendshapes
        device = "cuda" if torch.cuda.is_available() else "cpu"
        generated_facial_data = generate_facial_data_from_bytes(
            wav_data, 
            blendshape_model, 
            device, 
            config
        )
        
        # Conversion si n√©cessaire
        if isinstance(generated_facial_data, np.ndarray):
            blendshapes = generated_facial_data.tolist()
        else:
            blendshapes = generated_facial_data
        
        # Envoi √† LiveLink
        if blendshapes:
            if isinstance(blendshapes, list):
                if isinstance(blendshapes[0], list):
                    # Multiple frames
                    for frame in blendshapes:
                        send_to_livelink_fast(frame)
                else:
                    # Frame unique
                    send_to_livelink_fast(blendshapes)
        
        return jsonify({'status': 'ok'})
    
    except Exception as e:
        logger.error(f"Erreur: {str(e)}")
        return jsonify({"status": "error", "message": str(e)}), 500

if __name__ == '__main__':
    print("\n" + "="*50)
    print("üöÄ API GALA - GPU 1")
    print("="*50 + "\n")
    
    # Initialisation
    load_neurosync_model()
    init_livelink()
    
    # Optimisations CUDA
    if torch.cuda.is_available():
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.backends.cudnn.benchmark = True
    
    print(f"API en √©coute sur le port {API_PORT}")
    print(f"LiveLink: {LIVELINK_IP}:{LIVELINK_PORT}")
    print(f"GPU utilis√©: {os.environ.get('CUDA_VISIBLE_DEVICES', 'default')}")
    print("\n" + "="*50 + "\n")
    
    app.run(host='0.0.0.0', port=API_PORT, debug=False, threaded=True)